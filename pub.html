<!--
Author: W3layouts
Author URL: http://w3layouts.com
License: Creative Commons Attribution 3.0 Unported
License URL: http://creativecommons.org/licenses/by/3.0/
-->

<!DOCTYPE html>
<html lang="zxx">

<head>
    <title>Fenggen Yu's publication</title>
    <!-- Meta tag Keywords -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8" />
    <script>
        addEventListener("load", function() {
            setTimeout(hideURLbar, 0);
        }, false);

        function hideURLbar() {
            window.scrollTo(0, 1);
        }

    </script>
    <!-- //Meta tag Keywords -->
    <!-- Custom-Files -->
    <link rel="stylesheet" href="css/bootstrap.css">
    <!-- Bootstrap-Core-CSS -->
    <link rel="stylesheet" href="css/style.css" type="text/css" media="all" />
    <link rel="stylesheet" href="css/slider.css" type="text/css" media="all" />
    <!-- Style-CSS -->
    <!-- font-awesome-icons -->
    <link href="css/font-awesome.css" rel="stylesheet">
    <!-- //font-awesome-icons -->
    <!-- /Fonts -->
    <link href="//fonts.googleapis.com/css?family=Catamaran:100,200,300,400,500,600,700,800" rel="stylesheet">

    <!-- //Fonts -->
</head>

<body>
	<!-- mian-content -->
    <div class="main-w3-pvt-header-sec page-w3pvt-inner" id="home">

        <!-- header -->
        <header>
            <div class="container">
                <div class="header d-lg-flex justify-content-between align-items-center py-lg-3 px-lg-3">
                    <!-- logo -->
                    <div id="logo">
                        <h1><a href="index.html">Fenggen Yu</a></h1>
                    </div>
                    <!-- //logo -->
                    <div class="w3pvt-bg">
                        <!-- nav -->
                        <div class="nav_w3pvt">
                            <nav>
                                <label for="drop" class="toggle">Menu</label>
                                <input type="checkbox" id="drop" />
                                <ul class="menu">
                                    <li><a href="index.html">Home</a></li>
                                    <li><a href="bio.html">Bio & cv</a></li>
                                    <li class="active"><a href="pub.html">Publications</a></li>
                                    <li><a href="contact.html">Contact Me</a></li>
                                </ul>
                            </nav>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <!-- //header -->
        <!-- //slider -->
    </div>
	
    <!-- /features -->
    <section class="about py-md-5 py-5" id="loans">
		
        <div class="container py-md-5">
			<h3>Publications</h3>

            <div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/LLGS.png" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title">Hao Sun, <strong>Fenggen Yu</strong>, Huiyao Xu, Tao Zhang, Changqing Zou.</h3>
                    <h3 class="mb-3 wthree_title">LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian Splatting for Novel View Synthesis.</h3>
                    <h3 class="mb-3 wthree_title"><a href="https://arxiv.org/abs/2504.10331">[Paper]</a>, Accepted to <a href="https://acmmm2025.org/">[ACM MM 2025]</a>.</h3>
					<p>We propose LL-Gaussian, a novel framework for 3D reconstruction and enhancement from low-light sRGB images, enabling pseudo normal-light novel view synthesis. </p>
                </div>
			</div>
            <div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/res.png" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title"><a href="https://suikei-wang.github.io/">Ruiqi Wang</a>, <a href="https://yi-ming-qian.github.io/">Yiming Qian</a>, <a href="https://kwang-ether.github.io/">Kai Wang</a>, <strong>Fenggen Yu</strong>, and <a href="https://www.cs.sfu.ca/~haoz/">Hao(Richard) Zhang</a>.</h3>
                    <h3 class="mb-3 wthree_title">RESAnything: Attribute Prompting for Arbitrary Referring Segmentation.</h3>
                    <h3 class="mb-3 wthree_title"><a href="./files/Neurips25_resanything.pdf">[Paper]</a>, Accepted to <a href="https://neurips.cc/">[NeurIPS 2025]</a>.</h3>
					<p>We propose an open-vocabulary and zero-shot method for arbitrary referring expression segmentation (RES), targeting more general input expressions than those handled by prior works. </p>
                </div>
			</div>
            
            <div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/CAGE.png" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title">Yifei Tong, Runze Tian, Xiao Han, Dingyao Liu, <strong>Fenggen Yu</strong>, Yan Zhang.</h3>
                    <h3 class="mb-3 wthree_title">CAGE-GS: High-fidelity Cage Based 3D Gaussian Splatting Deformation.</h3>
                    <h3 class="mb-3 wthree_title"><a href="https://arxiv.org/abs/2504.12800">[Paper]</a></h3>
					<p>We introduce CAGE-GS, a cage-based 3DGS deformation method that seamlessly aligns a source 3DGS scene with a user-defined target shape. Our approach learns a deformation cage from the target, which guides the geometric transformation of the source scene.</p>
                </div>
			</div>

            <div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/ARAP.png" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title">Xiao Han, Runze Tian, Yifei Tong, <strong>Fenggen Yu</strong>, Dingyao Liu, Yan Zhang.</h3>
                    <h3 class="mb-3 wthree_title">ARAP-GS: Drag-driven As-Rigid-As-Possible 3D Gaussian Splatting Editing with Diffusion Prior.</h3>
                    <h3 class="mb-3 wthree_title"><a href="https://arxiv.org/abs/2504.12788">[Paper]</a></h3>
					<p>We introduce ARAP-GS, a drag-driven 3DGS editing framework based on As-Rigid-As-Possible (ARAP) deformation. Unlike previous 3DGS editing methods, we are the first to apply ARAP deformation directly to 3D Gaussians, enabling flexible, drag-driven geometric transformations.</p>
                </div>
			</div>

            <div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/DPA.png" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title"><strong>Fenggen Yu</strong>, <a href="https://yi-ming-qian.github.io/">Yiming Qian</a>, <a href="https://xu-zhang-1987.github.io/">Xu Zhang</a>, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, and <a href="https://www.cs.sfu.ca/~haoz/">Hao(Richard) Zhang</a>.</h3>
                    <h3 class="mb-3 wthree_title">DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable Primitive Assembly.</h3>
                    <h3 class="mb-3 wthree_title"><a href="https://arxiv.org/pdf/2404.00875">[Paper]</a>, Accepted to <a href="https://eccv2024.ecva.net/">[ECCV 2024]</a>.</h3>
					<p>We present a differentiable rendering framework to learn structured 3D abstractions in the form of primitive assemblies from sparse RGB images capturing a 3D object.</p>
                </div>
			</div>

            <div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/sweep.png" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title"><a href="https://mingrui-zhao.github.io/">Mingrui Zhao</a>, <a href="https://actasidiot.github.io/">Yizhi Wang</a>, <strong>Fenggen Yu</strong>, and <a href="https://sites.google.com/site/alimahdaviamiri/">Ali Mahdavi-Amiri</a>.</h3>
                    <h3 class="mb-3 wthree_title">SweepNet: Unsupervised Learning Shape Abstraction via Neural Sweepers.</h3>
                    <!-- <h3 class="mb-3 wthree_title"><a href="https://arxiv.org/abs/2303.11530v1">[Paper]</a>, Accepted to <a href="https://eccv2024.ecva.net/">[ECCV 2024]</a>.</h3> -->
                    <h3 class="mb-3 wthree_title">Accepted to <a href="https://eccv2024.ecva.net/">[ECCV 2024]</a>.</h3>
					<p>Sweep surfaces, commonly found in human-made objects, aid in this process by effectively capturing and representing object geometry, thereby facilitating abstraction. In this paper, we introduce SweepNet, a novel approach to shape abstraction through sweep surfaces.</p>
                    
                </div>
			</div>

            <div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/AL_IntPart_long.png" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title"><a href="https://suikei-wang.github.io/">Ruiqi Wang</a>, <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>, <strong>Fenggen Yu</strong>, and <a href="https://www.cs.sfu.ca/~haoz/">Hao(Richard) Zhang</a>.</h3>
                    <h3 class="mb-3 wthree_title">Coarse-to-Fine Active Segmentation of Interactable Parts in Real Scene Images.</h3>
                    <h3 class="mb-3 wthree_title"><a href="https://arxiv.org/abs/2303.11530v1">[Paper]</a>, Accepted to <a href="https://eccv2024.ecva.net/">[ECCV 2024]</a>.</h3>
                    
					<p>We introduce the first active learning (AL) framework for high-accuracy instance segmentation of dynamic, interactable parts from RGB images of real indoor scenes.</p>
                    
                </div>
			</div>

            <div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/dualcsg.png" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title"><strong>Fenggen Yu</strong>, <a href="https://qiminchen.github.io/">Qimin Chen</a>, <a href="http://mtanveer.com/">Maham Tanveer</a>, <a href="https://sites.google.com/site/alimahdaviamiri/">Ali Mahdavi-Amiri</a>, and <a href="https://www.cs.sfu.ca/~haoz/">Hao(Richard) Zhang</a>.</h3>
                    <h3 class="mb-3 wthree_title">D<sup>2</sup>CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts.</h3>
                    <h3 class="mb-3 wthree_title"><a href="https://arxiv.org/abs/2301.11497">[Paper]</a>, Accepted to <a href="https://nips.cc/">[NeurIPS 2023]</a>. </h3>
                    
					<p>We present D<sup>2</sup>CSG, a neural model composed of two dual and complementary network branches, with dropouts, for unsupervised learning of compact constructive solid geometry (CSG) representations of 3D CAD shapes.</p>
                    
                </div>
			</div>

            <div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/hal3d.png" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title"><strong>Fenggen Yu</strong>, <a href="https://yi-ming-qian.github.io/">Yiming Qian</a>, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, and <a href="https://www.cs.sfu.ca/~haoz/">Hao(Richard) Zhang</a>.</h3>
                    <h3 class="mb-3 wthree_title">HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling.</h3>
                    <h3 class="mb-3 wthree_title"><a href="https://arxiv.org/abs/2301.10460">[Paper]</a>, Accepted to <a href="https://iccv2023.thecvf.com/">[ICCV 2023]</a>.</h3>
					<p>We present the first active learning tool for fine-grained 3D part labeling, a problem which challenges even the most advanced deep learning (DL) methods due to the significant structural variations among the small and intricate parts.</p>
                </div>
			</div>
            
            <div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/CAPRI.png" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title"><strong>Fenggen Yu</strong>, <a href="https://czq142857.github.io/">Zhiqin Chen</a>, <a href="https://manyili12345.github.io/">Manyi Li</a>, Aditya Sanghi, Hooman Shayani, <a href="https://sites.google.com/site/alimahdaviamiri/">Ali Mahdavi-Amiri</a> and <a href="https://www.cs.sfu.ca/~haoz/">Hao(Richard) Zhang</a>.</h3>
                    <h3 class="mb-3 wthree_title">CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly.</h3>
                    <h3 class="mb-3 wthree_title"><a href="https://arxiv.org/abs/2104.05652">[Paper]</a>, <a href="capri.html">[Project Page]</a>, Accepted to <a href="https://cvpr2022.thecvf.com/">[CVPR 2022]</a> </h3>
                    
					<p>We introduce CAPRI-Net, a neural network for learning compact and interpretable implicit representations of 3D computer-aided design (CAD) models, in the form of adaptive primitive assemblies.</p>
                    
                </div>
			</div>

            <div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/RaidaR.png" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title">Jiongchao Jin, Arezou Fatemi, Wallace Lira, <strong>Fenggen Yu</strong>, Biao Leng, <a href="https://maruitx.github.io/">Rui Ma</a>, <a href="https://sites.google.com/site/alimahdaviamiri/">Ali Mahdavi-Amiri</a> and <a href="https://www.cs.sfu.ca/~haoz/">Hao(Richard) Zhang</a>.</h3>
                    <h3 class="mb-3 wthree_title">RaidaR: A Rich Annotated Image Dataset of Rainy Street Scenes.</h3>
                    <h3 class="mb-3 wthree_title"><a href="https://arxiv.org/abs/2104.04606">[Paper]</a>, Second ICCV Workshop on Autonomous Vehicle Vision (AVVision), 2021</h3>
                    
					<p>We introduce RaidaR, a rich annotated image dataset of rainy street scenes, to support autonomous driving research. The new dataset contains the largest number of rainy images (58,542) to date, 5,000 of which provide semantic segmentations and 3,658 provide object instance segmentations.</p>
                    
                </div>
			</div>
			
			<div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/VDAC.png" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title"><a href="https://sites.google.com/site/alimahdaviamiri/">Ali Mahdavi-Amiri</a>, <strong>Fenggen Yu</strong>, <a href="https://homes.cs.washington.edu/~haisen/">Haisen Zhao</a>, <a href="https://homes.cs.washington.edu/~adriana/">Adriana Schulz</a> and <a href="https://www.cs.sfu.ca/~haoz/">Hao(Richard) Zhang</a>.</h3>
                    <h3 class="mb-3 wthree_title">VDAC: Volume Decompose-and-Carve for Subtractive Manufacturing.</h3>
                    <h3 class="mb-3 wthree_title">Accepted to <a href="https://sa2020.siggraph.org/en/">SIGGRAPH ASIA 2020</a>|<a href="https://drive.google.com/file/d/1TOxxNuAeYSceriNUQnvACm_oxFEo_kF3/view">[Paper]</a>|<a href="https://sites.google.com/site/alimahdaviamiri/projects/vdac?authuser=0">[Project page]</a></h3>
                    
					<p>We introduce carvable volume decomposition for efficient 3-axis CNC machining of 3D freeform objects, where our goal is to develop a fully automatic method to jointly optimize setup and path planning.</p>
                    
                </div>
			</div>
            <div class="feature-grids row mt-3">
                <div class="col-lg-4 ab-content-img">
                    <img src="images/proj1.jpg" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title"><strong>Fenggen Yu</strong>, Kun Liu, <a href="https://cs.nju.edu.cn/zhangyan/">Yan Zhang</a>, <a href="http://www.sfu.ca/~cza68/">Chenyang Zhu</a>, <a href="http://kevinkaixu.net/index.html">Kai Xu</a>.</h3>
                    <h3 class="mb-3 wthree_title">PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation.</h3>
                    <h3 class="mb-3 wthree_title">Accepted to <a href="cvpr2019.thecvf.com/">CVPR 2019</a>|<a href="https://arxiv.org/abs/1903.00709">[Paper]</a>|<a href="https://github.com/FoggYu/PartNet">[Code & data]</a></h3>
                    
					<p>Deep learning approaches to 3D shape segmentation are typically formulated as a multi-class labeling problem. Existing models are trained for a fixed set of labels, which greatly limits their flexibility and adaptivity. We opt for topdown recursive decomposition and develop the first deep learning model for hierarchical segmentation of 3D shapes, based on recursive neural networks.</p>
                    
                </div>
			</div>
			<div class="feature-grids row mt-3">
				<div class="col-lg-4 ab-content-img">
                    <img src="images/proj2.jpg" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title"><strong>Fenggen Yu</strong>, <a href="https://cs.nju.edu.cn/zhangyan/">Yan Zhang</a>, <a href="http://kevinkaixu.net/index.html">Kai Xu</a>, <a href="https://sites.google.com/site/alimahdaviamiri/">Ali Mahdavi-Amiri</a> and <a href="https://www.cs.sfu.ca/~haoz/">Hao(Richard) Zhang</a>.</h3>
                    <h3 class="mb-3 wthree_title">Semi-Supervised Co-Analysis of 3D Shape Styles from Projected Lines.</h3>
                    <h3 class="mb-3 wthree_title">Accepted to <a href="https://tog.acm.org/">ACM Transactions on Graphics</a> (to be presented at SIGGRAPH 2018), 37(2)|<a href="https://arxiv.org/abs/1804.06579">[Paper]</a>|<a href="https://github.com/FoggYu/proj_style">[Code & data]</a>.</h3>
                    
					<p>We present a semi-supervised co-analysis method for learning 3D shape styles from projected feature lines, achieving style patch localization with only weak supervision. Given a collection of 3D shapes spanning multiple object categories and styles, we perform style co-analysis over projected feature lines of each 3D shape and then backproject the learned style features onto the 3D shapes.</p>
                </div>
			</div>
			
			<div class="feature-grids row mt-3">
				<div class="col-lg-4 ab-content-img">
                    <img src="images/proj4.jpg" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title">PanPan Shui, Pengyu Wang, <strong>Fenggen yu</strong>, Bingyang Hu, Yuan Gan, Kun Liu, <a href="https://cs.nju.edu.cn/zhangyan/">Yan Zhang</a>.</h3>
                    <h3 class="mb-3 wthree_title">3D Shape Segmentation Based on Viewpoint Entropy and Projective Fully Convolutional Networks Fusing Multi-view Features.</h3>
                    <h3 class="mb-3 wthree_title">Accepted to <a href="http://www.icpr2018.net/">ICPR 2018</a>|<a href="https://www.researchgate.net/publication/329316036_3D_Shape_Segmentation_Based_on_Viewpoint_Entropy_and_Projective_Fully_Convolutional_Networks_Fusing_Multi-view_Features">[Paper]</a>.</h3>
                    
					<p>This paper introduces an architecture for segmenting 3D shapes into labeled semantic parts. Our architecture
					combines viewpoint selection method based on viewpoint entropy,
					multi-view image-based Fully Convolutional Networks (FCNs)
					and graph cuts optimization method to yield coherent segmentation of 3D shapes. </p>
                </div>
			</div>
			
			<div class="feature-grids row mt-3">
				<div class="col-lg-4 ab-content-img">
                    <img src="images/proj3.jpg" alt="" class="img-fluid image1">
                </div>
                <div class="col-lg-8 ab-content-inf pl-lg-5">
					<h3 class="mb-3 wthree_title">Pengyu Wang*, Yuan Gan*, Panpan Shui, <strong>Fenggen Yu</strong>, <a href="https://cs.nju.edu.cn/zhangyan/">Yan Zhang</a>, Songle Chen, <a href="https://cs.nju.edu.cn/sunzhx/">Zhengxing Sun</a>.</h3>
                    <h3 class="mb-3 wthree_title">3D Shape Segmentation via Shape Fully Convolutional Networks.</h3>
                    <h3 class="mb-3 wthree_title">Accepted to <a href="https://www.journals.elsevier.com/computers-and-graphics">Computer & Graphics</a>, Vol 70, Feb 2018.|<a href="https://arxiv.org/pdf/1702.08675.pdf">[Paper]</a>|<a href="https://github.com/yuangan/3D-Shape-Segmentation-via-Shape-Fully-Convolutional-Networks">[Code & data]</a>.</h3>
                    
					<p>We design a novel fully convolutional network architecture for shapes, denoted by Shape Fully Convolutional Networks (SFCN). 3D shapes are represented as graph structures in the SFCN architecture, based on novel graph convolution and pooling operations, which are similar to convolution and pooling operations used on images. </p>
                </div>
			</div>
        </div>
    </section>

    <!-- //footer -->
    <div class="copy-right">
        <div class="container">
            <div class="row">
				<p class="copy-right-grids text-md-left text-center my-sm-4 my-4 col-md-6">© All Rights Reserved | Designed by Fenggen Yu</p>
				<div class="w3-pvt-footer text-md-right text-center mt-4 col-md-5">
                    <ul class="list-unstyled w3-pvt-icons">
                        <li>
                            <a href="https://twitter.com/fenggenyu">
                                <span class="fa fa-twitter"></span>
                            </a>
                        </li>
                    </ul>
                </div>
                <div class="move-top text-right col-md-1"><a href="#home" class="move-top"> <span class="fa fa-angle-up  mb-3" aria-hidden="true"></span></a></div>

            </div>
        </div>
    </div>
</body>

</html>
